{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep this\n",
    "import os\n",
    "import pickle \n",
    "import numpy as np\n",
    "import argparse\n",
    "import  nltk \n",
    "import time\n",
    "from nltk import Tree, word_tokenize\n",
    "import numpy as np\n",
    "from nltk.grammar import Production\n",
    "import re\n",
    "from PYEVALB import parser as evalbparser\n",
    "# Here compute all util functions\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import string\n",
    "from collections import Counter , defaultdict\n",
    "from itertools import product "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/fatmamoalla/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # in requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process corpus =====\n",
      "Corpus size = 3099\n",
      "Train = 2479 Val = 310 Test = 310\n",
      "Finihed in : 1.1513080596923828\n",
      "Build vocabulary =====\n",
      "Training Vocabulary size =  8958\n",
      "Finihed in : 0.06253600120544434\n"
     ]
    }
   ],
   "source": [
    "train_size, test_size = 0.8,0.1\n",
    "path_corpus = 'sequoia-corpus+fct.mrg_strict'\n",
    "path_polyglot = 'polyglot-fr.pkl'\n",
    "## In main\n",
    "print('Process corpus =====')\n",
    "start =time.time()\n",
    "corpus = get_all_corpus(path=path_corpus)\n",
    "print('Corpus size =',len(list(corpus.keys())))\n",
    "tokenized_corpus = [Tree.fromstring(sentence).leaves() for sentence in corpus.values()]\n",
    "train, val, test = split_train_test(tokenized_corpus, train_size=0.8, val_size=0.1, test_size=0.1)\n",
    "# Here the corpus contains tags \n",
    "train_corpus, val_corpus , test_corpus = split_train_test(list(corpus.values()), train_size=0.8, val_size=0.1, test_size=0.1)\n",
    "print( 'Train =',len(train),'Val =',len(val),'Test =',len(test))\n",
    "end = time.time()\n",
    "print('Finihed in :',end-start)\n",
    "print('Build vocabulary =====')\n",
    "start =time.time()\n",
    "vocab = get_vocabulary (train)\n",
    "print('Training Vocabulary size = ',len(vocab))\n",
    "word2id = {word: idx for (idx, word) in enumerate(vocab)}\n",
    "id2word = dict(enumerate(vocab))\n",
    "end = time.time()\n",
    "print('Finihed in :',end-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers function \n",
    "\n",
    "### Get corpus functions\n",
    "def get_all_corpus(path= 'sequoia-corpus+fct.mrg_strict'):\n",
    "    \"\"\" Extract the input corpus of sentences and remove hyphen tags \"\"\"\n",
    "    full_path = path\n",
    "    corpus = {}\n",
    "    with open(full_path,encoding='utf-8') as f:\n",
    "        for i , line in enumerate(f):\n",
    "            sent =line.rstrip().split(\" \")\n",
    "            sent=[word.split(\"-\")[0] if word[0]=='(' else word for word in sent]\n",
    "            corpus[i]=\" \".join(sent)\n",
    "    return corpus\n",
    "def split_train_test(corpus, train_size=0.8, val_size=0.1, test_size=0.1):\n",
    "    \"\"\" Split the corpus into : train (80%),val(10%), test(10%) \"\"\"\n",
    "    # Maybe add shuffle\n",
    "    \n",
    "    n = len(corpus)\n",
    "    train_idx = int(n * train_size)\n",
    "    val_idx = int(n*(val_size+train_size))\n",
    "    train = corpus [:train_idx]\n",
    "    val = corpus [train_idx:val_idx]\n",
    "    test = corpus [val_idx:]\n",
    "    return train, val, test\n",
    "\n",
    "def get_vocabulary (train_corpus):\n",
    "    vocab = []\n",
    "    for sentence in train_corpus : \n",
    "         vocab.extend(sentence)\n",
    "    vocab = np.unique(vocab)\n",
    "    return vocab\n",
    "\n",
    "### Get embeddings from polyglot\n",
    "def get_fr_word_embedding(vocab,path = 'polyglot-fr.pkl'):\n",
    "    \"\"\" From the Fr plyglot lexicon, extrcat words and embeddings\"\"\"\n",
    "    full_path = path\n",
    "    with open(full_path, 'rb') as f:\n",
    "        polyglot = pickle._Unpickler(f)\n",
    "        polyglot.encoding = 'latin1'\n",
    "        words, embeddings =   polyglot.load()\n",
    "        #all_words = np.array(list(set(vocab)&set(words)))\n",
    "        w2embed = {word:embedding for word, embedding in zip(words, embeddings)}\n",
    "        \n",
    "    return w2embed\n",
    "\n",
    "### Distances definition and neighbors\n",
    "\n",
    "\n",
    "def cosine_similarity(embed_vec,w2embed):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between embed_vec(current embedding word)\n",
    "    \"\"\"\n",
    "    embeddings =list(w2embed.values())\n",
    "    inner_embed = np.inner(embeddings,embed_vec)\n",
    "    sim_score = inner_embed / (np.linalg.norm(embed_vec)*np.linalg.norm(embeddings,axis=1))\n",
    "    return sim_score\n",
    "\n",
    "def closest_word_embed(word, w2embed, id2word_plyglot, embed_neigh=10):\n",
    "    if word in w2embed.keys():\n",
    "        \n",
    "        vector = w2embed[word]\n",
    "        similar_vectors  = cosine_similarity(vector,w2embed)\n",
    "        candidates_id = np.flip(np.argsort(similar_vectors)[-embed_neigh:])\n",
    "        embed_candidates = [id2word_plyglot[idx] for idx in candidates_id ]\n",
    "        return embed_candidates\n",
    "      \n",
    "    else:\n",
    "        return []\n",
    "\n",
    "### OOV replacement \n",
    "def levenstein_distance(sent1,sent2):\n",
    "    \"\"\"\n",
    "    Computes the levenstein distance between two sentences\n",
    "    \"\"\"\n",
    "    n,p = len(sent1)+1 , len(sent2)+1\n",
    "    m = np.zeros((n,p))\n",
    "    m[:,0] = np.arange(n)\n",
    "    m[0,:] = np.arange(p)\n",
    "    for i in range(1,n):\n",
    "        for j in range(1,p):\n",
    "            if  sent1[i-1] == sent2[j-1]:\n",
    "                m[i,j] = min([m[i-1,j]+1,m[i,j-1]+1,m[i-1,j-1]])\n",
    "            else : \n",
    "                m[i,j] = min([m[i-1,j]+1,m[i,j-1]+1,m[i-1,j-1]+1])\n",
    "    \n",
    "    return m[n-1,p-1]\n",
    "\n",
    "def levenstein_candidates(word,vocab,lev_neigh = 5):\n",
    "    \"\"\" \n",
    "    Find the corespondant word for each oov word\n",
    "    \"\"\"  \n",
    "    lev =np.vectorize(lambda w : levenstein_distance(word.lower(),w.lower()))\n",
    "    voba_lev_distances = lev(vocab)\n",
    "    result = [new_word[1] for new_word in sorted(zip(voba_lev_distances,vocab))[:lev_neigh]]\n",
    "    ## result is a word\n",
    "    return result\n",
    "\n",
    "def update_vocab_embeddings(word2id, w2embed):\n",
    "    inter = set(word2id.keys()).intersection(set(w2embed.keys()))\n",
    "    inter_word2id = {w : word2id[w] for w in inter}\n",
    "    size_vocab = len(list(inter_word2id.keys()))\n",
    "    inter_w2embed = {w : w2embed[w] for w in inter}\n",
    "    size_embed = len(list(inter_w2embed.keys()))\n",
    "    print('Intersection between vocab and embeddings', 'new_vocab_size=',size_vocab, 'new_embed_size=',)\n",
    "    return inter_word2id, inter_w2embed\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get embedding from polyglot Fr =====\n",
      "Finihed in : 95.75415396690369\n",
      "Intersection between vocab and embeddings new_vocab_size= 6951 new_embed_size=\n"
     ]
    }
   ],
   "source": [
    "# !!! takes ~1-2min to run !!!\n",
    "print('Get embedding from polyglot Fr =====')\n",
    "start =time.time()\n",
    "w2embed = get_fr_word_embedding(vocab,path = path_polyglot)\n",
    "id2word_plyglot = dict(enumerate(w2embed.keys()))\n",
    "end =time.time()\n",
    "print('Finihed in :',end-start)\n",
    "### Update vocab, embed\n",
    "\n",
    "inter_word2id, inter_w2embed =update_vocab_embeddings(word2id, w2embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorporating context: bigrams  =====\n",
      "Build Unigrams from train ===\n",
      "Build Bigrams from train ===\n",
      "length of unigrams/bigrams: 8958\n"
     ]
    }
   ],
   "source": [
    "### OOV module\n",
    "print('Incorporating context: bigrams  =====')\n",
    "unigrams, bigrams = uni_bi_grams(vocab, train, word2id)\n",
    "\n",
    "print('length of unigrams/bigrams:',len(unigrams))\n",
    "# check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def uni_bi_grams(vocab, sentences, word2id):\n",
    "    n= len(vocab)\n",
    "    bigrams= np.ones((n,n))\n",
    "    unigrams= np.zeros(n)\n",
    "    print('Build Unigrams from train ===')\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            unigrams[word2id[word]] +=1\n",
    "    norm_uni = np.sum(unigrams)\n",
    "    unigrams/=norm_uni\n",
    "    \n",
    "    print('Build Bigrams from train ===')\n",
    "    for sentence in sentences:\n",
    "        for i,word in enumerate(sentence):\n",
    "            bigrams[word2id[sentence[i-1]],word2id[word]] +=1\n",
    "            \n",
    "    norm_bi = np.sum(bigrams, axis = 1)[:, None]\n",
    "    bigrams/=norm_bi\n",
    "    return unigrams, bigrams \n",
    "\n",
    "def score_context(idx, word, sentence,word2id,unigrams, bigrams,coef =0.2): # Process_word function\n",
    "    \n",
    "    # We take the log to avoid overflow\n",
    "    if idx ==0:\n",
    "        return np.log(unigrams[word2id[word]])\n",
    "    else: \n",
    "        previous_word = sentence[-1]\n",
    "        score = coef* unigrams[word2id[word]] + (1-coef)*bigrams[word2id[previous_word],word2id[word]]\n",
    "        \n",
    "        return np.log(score)\n",
    "    \n",
    "def get_new_words(word,vocab,w2embed,word2id,unigrams, bigrams,id2word_plyglot,lev_neigh=10,embed_neigh=20):\n",
    "    candidates =[]\n",
    "    max_iter = 20\n",
    "    i=0\n",
    "    while len(candidates)==0 and i<max_iter :\n",
    "        lev_list = levenstein_candidates(word,vocab,lev_neigh)\n",
    "        embed_list = closest_word_embed(word, w2embed, id2word_plyglot, embed_neigh)\n",
    "        candidates = set(embed_list).intersection(set(lev_list)) \n",
    "        lev_neigh+=1\n",
    "        i+=1\n",
    "    return candidates\n",
    "    \n",
    "\n",
    "\n",
    "# In helpers\n",
    "def OOV(sentence , vocab, w2embed,word2id,unigrams, bigrams,id2word_plyglot,lev_neigh=10,embed_neigh=20):\n",
    "    score =0\n",
    "    replacement = []\n",
    "    tokens = Tree.fromstring(sentence).leaves()\n",
    "    #print(tokens)\n",
    "    \n",
    "    for (idx,word) in enumerate(tokens):\n",
    "            \n",
    "            if word in vocab:\n",
    "                #print('word in vocab = ',word)\n",
    "                score += score_context(idx, word, replacement,word2id,unigrams,bigrams,coef =0.2)\n",
    "                replacement.append(word)   \n",
    "                \n",
    "            else:   \n",
    "                #print('word not in vocab = ',word)\n",
    "                correction =[]\n",
    "                candidates = get_new_words(word,vocab,w2embed,word2id,unigrams, bigrams,id2word_plyglot,lev_neigh=10,embed_neigh=20)\n",
    "                for new_word in candidates :\n",
    "                    correction.append([new_word,score_context(idx, new_word, replacement,word2id,unigrams, bigrams,coef =0.2)])\n",
    "                \n",
    "                if len(correction)>0:\n",
    "                    best_combination = sorted(correction)[-1]\n",
    "                    score+=  best_combination[1] \n",
    "                    replacement.append(best_combination[0])\n",
    "                    #print('replacement',replacement)\n",
    "                #else:print('No replacment is found')\n",
    "    return \" \".join(replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( (SENT (PONCT -) (NP (ADJ 19) (NC janvier) (NC 2004)) (PONCT :) (NP (DET le) (NC juge) (NPP Armand) (NPP Riberolles)) (VN (V clôt)) (NP (DET ses) (NC investigations) (COORD (CC et) (NP (DET l') (NC instruction) (PP (P de) (NP (DET l') (NC affaire) (PP (P+D des) (NP (NC HLM) (PP (P de) (NP (NPP Paris)))))))))) (PONCT .)))\n",
      "['-', '19', 'janvier', '2004', ':', 'le', 'juge', 'Armand', 'Riberolles', 'clôt', 'ses', 'investigations', 'et', \"l'\", 'instruction', 'de', \"l'\", 'affaire', 'des', 'HLM', 'de', 'Paris', '.']\n",
      "word in vocab =  -\n",
      "word in vocab =  19\n",
      "word in vocab =  janvier\n",
      "word in vocab =  2004\n",
      "word in vocab =  :\n",
      "word in vocab =  le\n",
      "word in vocab =  juge\n",
      "word not in vocab =  Armand\n",
      "New candidates are found\n",
      "word not in vocab =  Riberolles\n",
      "No replacement found\n",
      "No replacment is found\n",
      "word not in vocab =  clôt\n",
      "No replacement found\n",
      "No replacment is found\n",
      "word in vocab =  ses\n",
      "word in vocab =  investigations\n",
      "word in vocab =  et\n",
      "word in vocab =  l'\n",
      "word in vocab =  instruction\n",
      "word in vocab =  de\n",
      "word in vocab =  l'\n",
      "word in vocab =  affaire\n",
      "word in vocab =  des\n",
      "word in vocab =  HLM\n",
      "word in vocab =  de\n",
      "word in vocab =  Paris\n",
      "word in vocab =  .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"- 19 janvier 2004 : le juge Edmond ses investigations et l' instruction de l' affaire des HLM de Paris .\""
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test : to delete\n",
    "sentence1 = test_corpus[0]\n",
    "print(sentence1)\n",
    "\n",
    "OOV(sentence1, vocab, w2embed,word2id,unigrams, bigrams,id2word_plyglot,lev_neigh=10,embed_neigh=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Almost ok : oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probabilities_lexicon(lexicon_list):\n",
    "    unique_pos, distinct = np.unique(lexicon_list, return_counts=True)\n",
    "    unique_pos = np.array([[pos.lhs(), pos.rhs()[0].lower()]  for pos in unique_pos])\n",
    "    pos_matrix = np.hstack((unique_pos,distinct.astype(np.float64).reshape(-1,1)))\n",
    "    \n",
    "    NT_l, x = np.unique(pos_matrix[:, 0], return_inverse=True)\n",
    "    NT_r, y = np.unique(pos_matrix[:, 1], return_inverse=True)\n",
    "    \n",
    "    l_size, r_size = len(NT_l),len(NT_r)\n",
    "    probabilities_lexicon= np.zeros((l_size, r_size))\n",
    "    probabilities_lexicon[x, y] = pos_matrix[:, 2]\n",
    "    probabilities_lexicon = probabilities_lexicon / np.sum(probabilities_lexicon,axis=1).reshape(-1,1)\n",
    "    return probabilities_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete\n",
    "def transform_probabilities(prob_dict,word2id,pos2id):\n",
    "    \n",
    "    distinct_pos = list(set(pos2id.keys()))\n",
    "    distinct_words = list(set(word2id.keys())) \n",
    "    \n",
    "    p,n = len(distinct_pos), len(distinct_words)\n",
    "    \n",
    "    word_tag_prob = np.zeros((p,n))\n",
    "    #word_tag_prob[np.array(map_pos.values()).reshape(-1,1),np.array(map_word.values()).reshape(-1,1)] = list(prob_dict.values())\n",
    "    \n",
    "    for word,idx_w in word2id.items():\n",
    "        for pos, idx_p in pos2id.items():\n",
    "            if (pos,word) in prob_dict.keys():\n",
    "                word_tag_prob[idx_p,idx_w] = prob_dict[(pos,word)]\n",
    "    \n",
    "    word_tag_prob= 1.* word_tag_prob/ np.sum( word_tag_prob, axis=1).reshape(-1,1)\n",
    "    return np.nan_to_num(word_tag_prob,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lexicon(train_corpus):\n",
    "    lexical_grammar =defaultdict(set)\n",
    "    axioms = set()\n",
    "    lexicon_list = []\n",
    "    start = time.time()\n",
    "    for sentence in train_corpus:\n",
    "        tree = Tree.fromstring(sentence, remove_empty_top_bracketing=True)\n",
    "        tree.chomsky_normal_form(horzMarkov=2)\n",
    "        tree.collapse_unary(collapsePOS=True, collapseRoot=True)\n",
    "        prods = tree.productions()\n",
    "        axioms.add(prods[0].lhs().symbol())\n",
    "        lexicon_list.extend([prod for prod in prods if prod.is_lexical()])\n",
    "        \n",
    "    lexicon_grammar= Counter(lexicon_list) # keys are unique and counts\n",
    "    rules_distinct = [[pos.lhs().symbol(),pos.rhs()[0].lower()] for pos in list(lexicon_grammar.keys())]\n",
    "    rules_count_distincts =  list(lexicon_grammar.values())\n",
    "    # Building lexical grammar \n",
    "    for rule in rules_distinct:\n",
    "        lexical_grammar[rule[0]].add(rule[1])\n",
    "    \n",
    "    probabilities_lexicon = get_probabilities_lexicon(lexicon_list)\n",
    "    \n",
    "    pos2id = {pos : idx for (idx,pos) in enumerate(lexical_grammar.keys())} # dict_pos_tags\n",
    "    return lexical_grammar, probabilities_lexicon,pos2id \n",
    "                \n",
    "                \n",
    "def PCFG_model(train_corpus)  :\n",
    "    pcfg_grammar_dict =defaultdict(set)\n",
    "    axioms = set()\n",
    "    pcfg_list = []\n",
    "    start = time.time()\n",
    "    for sentence in train_corpus:\n",
    "        tree = Tree.fromstring(sentence, remove_empty_top_bracketing=True)\n",
    "        tree.chomsky_normal_form(horzMarkov=2)\n",
    "        tree.collapse_unary(collapsePOS=True, collapseRoot=True)\n",
    "        prods = tree.productions()\n",
    "        axioms.add(prods[0].lhs().symbol())\n",
    "        pcfg_list.extend([prod for prod in prods if prod.is_nonlexical()])\n",
    "    pcfg_grammar= Counter(pcfg_list) # keys are unique and counts\n",
    "    rules_distinct = [[pos.lhs().symbol(),pos.rhs()] for pos in list(pcfg_grammar.keys())]\n",
    "    rules_count_distincts =  list(pcfg_grammar.values())\n",
    "    # Building pcfg grammar \n",
    "    for rule in rules_distinct:\n",
    "        pcfg_grammar_dict[rule[0]].add(rule[1])   \n",
    "    unique_rules, distinct = np.unique(pcfg_list, return_counts=True)\n",
    "    unique_rules = np.array([[rule.lhs(), rule.rhs()]  for rule in unique_rules])\n",
    "    rules_matrix = np.hstack((unique_rules,distinct.astype(np.float64).reshape(-1,1)))\n",
    "    NT_l, x = np.unique(rules_matrix[:, 0], return_inverse=True)\n",
    "    NT_r, y = np.unique(rules_matrix[:, 1], return_inverse=True)\n",
    "    l_size, r_size = len(NT_l),len(NT_r)\n",
    "    pcfg= np.zeros((l_size, r_size))\n",
    "    pcfg[x, y] = rules_matrix[:, 2]\n",
    "    pcfg = pcfg / np.sum(pcfg,axis=1).reshape(-1,1)\n",
    "    NT_lhs= {NT.symbol():idx for (idx,NT) in enumerate(NT_l)}\n",
    "    NT_rhs = {(NT[0].symbol(),NT[1].symbol()):idx for idx,NT in enumerate(NT_r) if len(NT)>1}\n",
    "    for (idx,NT) in enumerate(NT_r):\n",
    "        if len(NT)==1:\n",
    "            NT_rhs[NT[0].symbol()]=idx\n",
    "    return pcfg_grammar,pcfg_grammar_dict,axioms ,pcfg,NT_lhs,NT_rhs\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCFG model finished in 9.752292156219482\n"
     ]
    }
   ],
   "source": [
    "# In main\n",
    "start = time.time()\n",
    "lexical_grammar, probabilities_lexicon,pos2id = extract_lexicon(train_corpus) \n",
    "pcfg_grammar,pcfg_grammar_dict,axioms,pcfg,NT_lhs,NT_rhs  = PCFG_model(train_corpus) # pcfg_grammar_dict ~ \n",
    "end = time.time()\n",
    "# this is ok\n",
    "print('PCFG model finished in', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers for CYK\n",
    "def build_binaries_unaries(pcfg_grammar_dict,NT_l,NT_r,pos2id):\n",
    "    ### Reformat binaries and unaries\n",
    "    binaries = {}\n",
    "    for lhs in pcfg_grammar_dict.keys() :\n",
    "        for rhs in pcfg_grammar_dict[lhs] :\n",
    "            if not rhs in binaries.keys() : binaries[rhs] = set()\n",
    "            binaries[rhs].add(lhs)\n",
    "    left_binary = set([bi[0] for bi in binaries.keys()])\n",
    "    right_binary = set([bi[1] for bi in binaries.keys()])\n",
    "    \n",
    "    unaries_target = set([target for target in NT_r.keys() if np.ndim(target)==0])\n",
    "    binaries_target = set(NT_r.keys()) - set(unaries_target)\n",
    "    \n",
    "    binaries_init = defaultdict(set)\n",
    "    binaries_inv = defaultdict(set)\n",
    "    \n",
    "    for NT,target in pcfg_grammar_dict.items():\n",
    "        binaries_tg = target & binaries_target\n",
    "        if binaries_tg:\n",
    "            binaries_init.update({NT:binaries_tg})\n",
    "    \n",
    "    for NT,targets in binaries_init.items():\n",
    "        for target in targets:\n",
    "            binaries_inv[target].add(NT)\n",
    "            \n",
    "    ### reformat Non-terminal words\n",
    "    new_nt_l =NT_l.copy()\n",
    "    new_nt_l.update({nt:i+len(NT_l) for (nt,i) in pos2id.items()})\n",
    "    new_nt_l_inv = {i:nt for (nt,i) in new_nt_l.items()}\n",
    "    return   binaries_inv,new_nt_l\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "def get_word_tag_dict(lexical_grammar):\n",
    "    word2pos = defaultdict(set)\n",
    "    for pos,tokens in lexical_grammar.items():\n",
    "        for word in tokens:\n",
    "            word2pos[word].add(pos)\n",
    "    return word2pos\n",
    "    \n",
    "\n",
    "def process_sentence(sentence,probabilities_lexicon,pos2id,word2id,word2pos):\n",
    "    # keep this\n",
    "    tokens = sentence.split(' ')\n",
    "    p=len(tokens)\n",
    "    score = [[{} for i in range(p+1)] for j in range(p+1)]\n",
    "    score_left = [[set() for i in range(p+1)] for j in range(p+1)]\n",
    "    score_right = [[set() for i in range(p+1)] for j in range(p+1)]\n",
    "    \n",
    "    for idx, word in enumerate(tokens):\n",
    "        w = word.lower()\n",
    "        for pos in word2pos[w]:#rule=POS, rule_to_word=lexicon_grammar_inv \n",
    "            score[idx][idx+1][pos] = probabilities_lexicon[pos2id[pos],word2id[word]] \n",
    "            if pos in left_dict: \n",
    "                score_left[idx][idx+1].add(pos)\n",
    "            if pos in right_dict :\n",
    "                score_right[idx][idx+1].add(pos)          \n",
    "    return score, score_left,score_right    \n",
    "\n",
    "\n",
    "\n",
    "def failure_msg(sentence):\n",
    "    msg = '(SENT '\n",
    "    \n",
    "    for word in sentence[:-1]:\n",
    "        msg+= '(NULL '+word+')'\n",
    "    msg+= '(NULL '+sentence[-1]+')'+')'\n",
    "    return msg\n",
    "\n",
    "\n",
    "\n",
    "def reconstruct_tree(back_tags, start,end, tokens,axioms,score,NT,n):\n",
    "    \"\"\"\n",
    "    Use dynamic programming to track back the tree : recursive implementation\n",
    "    \"\"\"\n",
    "    if n==1:\n",
    "        candidates = [score[start][end].get(c,0) for c in axioms]\n",
    "        NT = axioms[np.argmax(np.array(candidates))]\n",
    "        if 'SENT' not in NT: return failure_msg(tokens)\n",
    "        msg = '(' + NT + ' ' + tokens[start] + ')'\n",
    "        return msg\n",
    "    \n",
    "    if end == start +1:\n",
    "        msg = '(' + NT + ' ' + tokens[start] + ')'\n",
    "        return msg\n",
    "       \n",
    "        \n",
    "    if end == n+start:\n",
    "        candidates =np.array([c for c in back_tags[start][end].keys() if c in axioms])\n",
    "        if not candidates: return failure_msg(tokens)\n",
    "        best_axiom = candidates[np.argmax([score[start][end][k] for k in candidates])]\n",
    "        limit,lhs, rhs = back_tags[start][end][best_axiom]\n",
    "    \n",
    "    else:\n",
    "        limit,lhs, rhs = back_tags[start][end][NT]\n",
    "     \n",
    "    left_result = '(' + NT + ' ' + reconstruct_tree(back_tags, start,limit, tokens,axioms,score,lhs,n) \n",
    "    right_result = reconstruct_tree(back_tags, limit, end, tokens,axioms,score,rhs,n) + ')'\n",
    "    msg = left_result + ' '+right_result \n",
    "    return msg\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "           \n",
    "    \n",
    "def unchomsky(parsing):\n",
    "    tree = Tree.fromstring(parsing)\n",
    "    tree.un_chomsky_normal_form()\n",
    "    unchomsky_result = ' '.join(tree.pformat().split())\n",
    "    return unchomsky_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CYK2(sentence,axioms, probabilities_lexicon,pos2id,word2id,lexical_grammar,pcfg,NT_lhs,NT_rhs,pcfg_grammar_dict):\n",
    "    \n",
    "    word2pos =  get_word_tag_dict(lexical_grammar)\n",
    "    binaries_inv, nt_dict = build_binaries_unaries(pcfg_grammar_dict,NT_lhs,NT_rhs,pos2id)\n",
    "    # not tokenized\n",
    "    score, score_left,score_right = process_sentence(sentence,probabilities_lexicon,pos2id,word2id,word2pos)\n",
    "    sentence= sentence.split(' ')\n",
    "    n= len(sentence)\n",
    "    back_tags =[[dict() for i in range(n+1)] for k in range(n+1)]# to check the length\n",
    "    \n",
    "    for w in range(2,n+1):\n",
    "        for start in range(n+1-w):\n",
    "            end= start+w\n",
    "            for limit in range(start+1,end):\n",
    "                ## O(n^3)\n",
    "                left_rule_set = score[start][limit]\n",
    "                right_rule_set = score[limit][end]\n",
    "                #l = [(A,B) for (A,B) in product(score_left[start][limit] , score_right[limit][end])]\n",
    "                #print(l)\n",
    "                couples= set(flat_binary)\n",
    "                #print('bi',couples)\n",
    "                intersection_rules = set(product(score_left[start][limit], score_right[limit][end])) & set(binaries_inv)\n",
    "                for (B,C) in intersection_rules:\n",
    "                    for A in binaries_inv[(B,C)] :\n",
    "                        proba = left_rule_set[B] * right_rule_set[C] * pcfg[NT_lhs[A]][NT_rhs[(B,C)]] # inspect dict_init_non_terminals and dict_target_non_terminals\n",
    "                        if proba > score[start][end].get(A, 0.):\n",
    "                            score[start][end][A] = proba\n",
    "                            if A in left_dict : \n",
    "                                score_left[start][end].add(A)\n",
    "                            if A in right_dict : \n",
    "                                score_right[start][end].add(A)\n",
    "                            \n",
    "                            back_tags[start,end,nt_dict[A]] = (limit,B,C) # inspect \n",
    "    \n",
    "    start ,end,NT =0,n,'SENT'\n",
    "    result = reconstruct_tree(back_tags, start,end, sentence,axioms,score,NT,n)\n",
    "    normalized_result = unchomsky(result)\n",
    "    return back_tags,normalized_result          \n",
    "                            \n",
    "                 \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results( vocab, w2embed,word2id,unigrams, bigrams,id2word_plyglot,axioms, probabilities_lexicon,pos2id,lexical_grammar,pcfg,NT_lhs,NT_rhs,pcfg_grammar_dict,sentence=None,testset=None,lev_neigh=10,embed_neigh=20):\n",
    "    \n",
    "    parsed_list=[]\n",
    "    if sentence:\n",
    "        print(sentence)\n",
    "        print('oov =====')\n",
    "        start =time.time()\n",
    "        tokens = sentence.split(' ') \n",
    "        # tokenized\n",
    "        # Don't split sentence\n",
    "        \n",
    "        replacement = OOV(sentence , vocab, w2embed,word2id,unigrams, bigrams,id2word_plyglot,lev_neigh,embed_neigh)\n",
    "        #replacement =sentence\n",
    "        print('replacement sentence',replacement)\n",
    "        # replacement is not tokenized\n",
    "        end = time.time()\n",
    "        #replacement = sentence\n",
    "        print('oov in =====',end-start)\n",
    "        parsed = CYK2(replacement,axioms, probabilities_lexicon,pos2id,word2id,lexical_grammar,pcfg,NT_lhs,NT_rhs,pcfg_grammar_dict)\n",
    "           #parsed = CYK2(replacement,pcfg,rule_to_word,binary_probs,axioms,left_dict, right_dict,couples)\n",
    "        parsed_list.append(parsed)\n",
    "    \n",
    "    if testset:\n",
    "        for sentence in tqdm(testset):\n",
    "            start =time.time()\n",
    "            print(sentence)\n",
    "            print('oov =====')\n",
    "            start =time.time()\n",
    "            tokens = sentence.split(' ') \n",
    "            # tokenized\n",
    "            # Don't split sentence\n",
    "        \n",
    "            replacement = OOV(sentence , vocab, w2embed,word2id,unigrams, bigrams,id2word_plyglot,lev_neigh,embed_neigh)\n",
    "            #replacement =sentence\n",
    "            print('replacement sentence',replacement)\n",
    "            # replacement is not tokenized\n",
    "            end = time.time()\n",
    "            #replacement = sentence\n",
    "            print('oov in =====',end-start)\n",
    "            parsed = CYK2(replacement,axioms, probabilities_lexicon,pos2id,word2id,lexical_grammar,pcfg,NT_lhs,NT_rhs,pcfg_grammar_dict)\n",
    "           #parsed = CYK2(replacement,pcfg,rule_to_word,binary_probs,axioms,left_dict, right_dict,couples)\n",
    "            parsed_list.append(parsed)\n",
    "    return parsed_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( (SENT (PONCT -) (NP (ADJ 19) (NC janvier) (NC 2004)) (PONCT :) (NP (DET le) (NC juge) (NPP Armand) (NPP Riberolles)) (VN (V clôt)) (NP (DET ses) (NC investigations) (COORD (CC et) (NP (DET l') (NC instruction) (PP (P de) (NP (DET l') (NC affaire) (PP (P+D des) (NP (NC HLM) (PP (P de) (NP (NPP Paris)))))))))) (PONCT .)))\n",
      "oov =====\n",
      "replacement sentence - 19 janvier 2004 : le juge Edmond ses investigations et l' instruction de l' affaire des HLM de Paris .\n",
      "oov in ===== 62.73377203941345\n"
     ]
    }
   ],
   "source": [
    "sentence0 = test_corpus[0]\n",
    "display_results( vocab, w2embed,word2id,unigrams, bigrams,id2word_plyglot,axioms, probabilities_lexicon,pos2id,lexical_grammar,pcfg,NT_lhs,NT_rhs,pcfg_grammar_dict,sentence=sentence0,testset=None)\n",
    "#display_results( axioms, prob_dict,pos2id,word2id,lexical_grammar,pcfg,NT_lhs,NT_rhs,sentence=sentence0,testset=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "\n",
    "***\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
